---
title: "Lab 08 - University of Edinburgh Art Collection"
author: "Conor Lacey"
date: "03-05-23"
output: github_document
---

### Load packages and data

```{r load-packages, message = FALSE}
suppressWarnings(library(tidyverse))
library(rvest)
library(skimr)
library(glue)
```


### Exercise 1

```{r first_url}
# set url
first_url <- "https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0"

# read html page
page <- read_html(first_url)

titles <- page %>%
  html_nodes(".iteminfo") %>%
  html_node("h3 a") %>%
  html_text() %>% 
  str_squish()

links <- page %>%
  html_nodes(".iteminfo") %>%   # same nodes
  html_node("h3 a") %>%         # as before
  html_attr("href") %>%         # but get href attribute instead of text
  str_replace(pattern =".", replacement = "https://collections.ed.ac.uk") #replacing
```

### Exercise 2

```{r Names}
names <- page %>% 
  html_nodes(".iteminfo") %>% 
  html_node(".artist") %>% 
  html_attr("title")
```

### Exercise 3

```{r uoe_art}
uoe_art <- tibble(title = titles, 
                  name = names,
                  link = links)
uoe_art
```

```{r load-data, message = FALSE, eval = FALSE}
# Remove eval = FALSE or set it to TRUE once data is ready to be loaded
uoe_art <- read_csv("data/uoe-art.csv")
```

### Exercise 4

```{r second_url}
# set url
second_url <- "https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=10"

# read html page
page <- read_html(second_url)

titles <- page %>%
  html_nodes(".iteminfo") %>%
  html_node("h3 a") %>%
  html_text() %>% 
  str_squish()

links <- page %>%
  html_nodes(".iteminfo") %>%   # same nodes
  html_node("h3 a") %>%         # as before
  html_attr("href") %>%         # but get href attribute instead of text
  str_replace(pattern =".", replacement = "https://collections.ed.ac.uk") #replacing

names <- page %>% 
  html_nodes(".iteminfo") %>% 
  html_node(".artist") %>% 
  html_attr("title")

second_ten <- tibble(title = titles, 
                  name = names,
                  link = links)
second_ten
```

### Exercises 5

```{r}
scrape_page <- function(url){
  
  # read page
  page <- read_html(url)
  
  # scrape titles
  titles <- page %>%
    html_nodes(".iteminfo") %>%
    html_node("h3 a") %>%
    html_text() %>% 
    str_squish()
  
  # scrape links
  links <- page %>%
    html_nodes(".iteminfo") %>%   # same nodes
    html_node("h3 a") %>%         # as before
    html_attr("href") %>%         # but get href attribute instead of text
    str_replace(pattern =".", replacement = "https://collections.ed.ac.uk") #replacing
  
  # scrape artists 
  names <- page %>% 
    html_nodes(".iteminfo") %>% 
    html_node(".artist") %>% 
    html_attr("title")
  
  # create and return tibble
  tibble(title = titles, 
         name = names,
         link = links)
  
}
```

### Exercise 6

``` {r test-function}
scrape_page(first_url)
scrape_page(second_url)
```

### Exercise 7

```{r URLs}
# list of urls to be scraped ---------------------------------------------------

root <- "https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset="
numbers <- seq(from = 0, to = 3010, by = 10)
urls <- glue("{root}{numbers}")

# map over all urls and output a data frame ------------------------------------

uoe_art <- map_dfr(urls, scrape_page)
```

### Exercise 8
```{r write-data-frame}
# write out data frame ---------------------------------------------------------
write_csv(uoe_art, file = "data/uoe-art.csv")
```

### Exercise 9

```{r separate-title-date, error = TRUE}


uoe_art <- uoe_art %>%
  separate(title, into = c("title", "date"), sep = "\\(") %>%
  mutate(year = str_remove(date, "\\)") %>% as.numeric()) %>%
  select(title, artist, year, ___)
```

### Exercise 10

Remove this text, and add your answer for Exercise 10 here.
Add code chunks as needed.
Don't forget to label your code chunk.
Do not use spaces in code chunk labels.

### Exercise 11

...

Add exercise headings as needed.
